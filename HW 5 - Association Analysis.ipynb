{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "\n",
    "Association analysis uses machine learning to extract hidden relationships from large datasets. In this assignment you'll be implementing two of the most commonly used algorithms for association rule mining: Apriori and FP-Growth.\n",
    "\n",
    "The dataset (`large_retail.txt`) that we are going to use for this assignment has been adapted from the [Retail Market Basket Dataset](http://fimi.ua.ac.be/data/retail.pdf). The dataset contains transaction records supplied by an anonymous Belgian retail supermarket store. Each line in the file represents a separate transaction with the item ids separated by space. The dataset has 3000 transaction records and 99 different item ids.\n",
    "\n",
    "We also provide a smaller dataset (`small_retail.txt`) with 9 transactions and 5 different item ids along with the solutions. *You should first test your implementation on this dataset, before running it on the larger dataset.*\n",
    "\n",
    "The assignment will be **autograded**. We will use the `diff` command in linux to compare the output files. So please **check your answers** based on the given sample output files.\n",
    "\n",
    "Implementation Hint:\n",
    "\n",
    "- Use the `frozenset` data structure in Python (similar to `set` in functionality) to represent the itemsets because `frozenset` is a hashable data structure. You can maintain a dictionary that maps from the itemset (a `frozenset`) to its support count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Apriori Algorithm\n",
    "\n",
    "Apriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. In this part, you'll be implementing this algorithm for generating the itemsets that occur more than the `min_sup` threshold. Based on these frequent itemsets you'll find association rules that have confidence above the `min_conf` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports (you can add additional headers if you wish)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        data = [[int(x) for x in line.rstrip().split()] for line in content]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 5],\n",
       " [2, 4],\n",
       " [2, 3],\n",
       " [1, 2, 4],\n",
       " [1, 3],\n",
       " [2, 3],\n",
       " [1, 3],\n",
       " [1, 2, 3, 5],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = load_dataset('small_retail.txt')\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Implement the function `create_1_itemsets` that takes as input the entire dataset and returns a list of all the 1-itemsets. For example, for `small_retail.txt` it should return:\n",
    "~~~\n",
    "[frozenset({1}),\n",
    " frozenset({2}),\n",
    " frozenset({3}),\n",
    " frozenset({4}),\n",
    " frozenset({5})]\n",
    " ~~~\n",
    " Please **don't hardcode** the item ids, your code should support item ids that are non-sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1_itemsets(dataset):\n",
    "    c1 = []\n",
    "    aux_list = []\n",
    "    # your code goes here\n",
    "    for transaction in dataset:\n",
    "        for x in transaction:\n",
    "            if x not in aux_list:\n",
    "                aux_list.append(x)\n",
    "    aux_list = sorted(aux_list)\n",
    "    for x in aux_list:\n",
    "        c1.append(frozenset([x]))\n",
    "    return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Implement function `filter_candidates` that takes as input the candidate itemsets, the dataset, and the minumum support count `min_sup`, and filters out candidates that don't meet the support threshold.\n",
    "\n",
    "**Hint:** You should also return the support count information (perhaps as a `dict`) for the itemsets. This will be useful later on for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates(candidates, dataset, min_sup):\n",
    "    retlist = []\n",
    "    support_data = {}\n",
    "    # your code goes here\n",
    "    for itemset in candidates:\n",
    "        for transaction in dataset:\n",
    "            if itemset.issubset(set(transaction)):\n",
    "                if itemset not in support_data:\n",
    "                    support_data[itemset] = 1\n",
    "                else:\n",
    "                    support_data[itemset] += 1\n",
    "    # Adding missing candidates\n",
    "    for itemset in candidates:\n",
    "        if itemset not in support_data:\n",
    "            support_data[itemset] = 0\n",
    "    # Choosing itemsets that have min_sup\n",
    "    for itemset in support_data:\n",
    "        if support_data[itemset] >= min_sup:\n",
    "            retlist.append(itemset)\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
      "[frozenset({1}), frozenset({2}), frozenset({3})]\n",
      "{frozenset({1}): 6, frozenset({2}): 7, frozenset({3}): 6, frozenset({4}): 2, frozenset({5}): 2}\n"
     ]
    }
   ],
   "source": [
    "candidates = create_1_itemsets(small_dataset)\n",
    "print(candidates)\n",
    "retlist, support_data = filter_candidates(candidates, small_dataset, 6)\n",
    "print(retlist)\n",
    "print(support_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Implement the function `generate_next_itemsets` that takes in frequent itemsets of size `k` and generates candidate itemsets of size `k + 1`.\n",
    "\n",
    "**Hint:** Use the fact that if `[1, 2, 3, 4]` is a frequent itemset of size 4 then `[1, 2, 3]` and `[1, 2, 4]` both will be frequent itemsets of size 3. You can use this to drastically reduce the number of candidate itemsets that you need to generate. (Use the F(k-1) x F(k-1) or F(k-1) x F(1) candidate generation method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1, 2, 3, 4})]\n"
     ]
    }
   ],
   "source": [
    "freq_sets = [frozenset([1,2,3]), frozenset([1,2,4])]\n",
    "\n",
    "x = generate_next_itemsets(freq_sets)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_itemsets(freq_sets):\n",
    "    retlist = []\n",
    "    aux_ret = []\n",
    "    aux_list = []\n",
    "    \n",
    "    # List of lists\n",
    "    for itemset in freq_sets:\n",
    "        itemset = sorted(itemset)\n",
    "        aux_list.append(itemset)\n",
    "    # Sorted list\n",
    "    aux_list = sorted(aux_list)\n",
    "    \n",
    "    # Fk-1 vs Fk-1\n",
    "    for lst in aux_list:\n",
    "        for vs in aux_list:\n",
    "            if (lst[:-1] == vs[:-1]) and (lst[-1]!=vs[-1]):\n",
    "                aux_ret.append(lst+[vs[-1]])\n",
    "    \n",
    "    # Back to frozenset\n",
    "    for lst in aux_ret:\n",
    "        if frozenset(lst) not in retlist:\n",
    "            retlist.append(frozenset(lst))\n",
    "    \n",
    "    return retlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Implement the function `apriori_freq_temsets` that takes the entire dataset as the input and returns the frequent itemsets that have support count more than `min_sup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_freq_temsets(dataset, minsup):\n",
    "    apriori_itemsets = []\n",
    "     # your code goes here\n",
    "    candidate_1_itemset = create_1_itemsets(dataset)\n",
    "    retlst, support_data = filter_candidates(candidate_1_itemset, dataset, minsup)\n",
    "    while len(retlst) > 0:\n",
    "        apriori_itemsets = apriori_itemsets + retlst\n",
    "        candidates = generate_next_itemsets(retlst)\n",
    "        retlst, support_data = filter_candidates(candidates, dataset, minsup)\n",
    "    return apriori_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Display the frequent item sets in the form of a table along with their `support_fraction` for the `large_retail.txt` dataset with min support count 300.\n",
    "\n",
    "Sample Table Format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.44\t[1, 2]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "`support_fraction(itemset) = support_count(itemset) / num_total_transactions`.\n",
    "\n",
    "Note that the `support_fraction` should be rounded to the nearest 2 decimal places (use `round(sup, 2)`). Also `support_fraction` and the itemset should be separated by a tab (`'\\t'`). The itemsets should also be in a sorted order where smaller itemsets should come before larger itemsets and itemsets of the same size should be sorted amongst themselves.\n",
    "\n",
    "For eg. \n",
    "~~~~\n",
    "[1, 2] should come before [1, 2, 3]\n",
    "[1, 2, 3] should come before [1, 2, 4]\n",
    "[1, 2, 3] should come before [1, 4, 5]\n",
    "[1, 2, 3] should come before [2, 3, 4]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The output also **shouldn't contain any duplicates**. The sample output for the `small_retail.txt` dataset with min support count as 2 is:\n",
    "\n",
    "~~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.78\t[2]\n",
    "0.67\t[3]\n",
    "0.22\t[4]\n",
    "0.22\t[5]\n",
    "0.44\t[1, 2]\n",
    "0.44\t[1, 3]\n",
    "0.22\t[1, 5]\n",
    "0.44\t[2, 3]\n",
    "0.22\t[2, 4]\n",
    "0.22\t[2, 5]\n",
    "0.22\t[1, 2, 3]\n",
    "0.22\t[1, 2, 5]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `apriori_itemsets.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_itemsets.txt` for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup\tFreq Itemset\n",
      "\n",
      "0.1\t[31]\n",
      "\n",
      "0.22\t[32]\n",
      "\n",
      "0.11\t[36]\n",
      "\n",
      "0.26\t[38]\n",
      "\n",
      "0.6\t[39]\n",
      "\n",
      "0.31\t[41]\n",
      "\n",
      "0.47\t[48]\n",
      "\n",
      "0.11\t[60]\n",
      "\n",
      "0.11\t[65]\n",
      "\n",
      "0.11\t[89]\n",
      "\n",
      "0.14\t[32, 39]\n",
      "\n",
      "0.12\t[32, 48]\n",
      "\n",
      "0.17\t[38, 39]\n",
      "\n",
      "0.11\t[38, 41]\n",
      "\n",
      "0.13\t[38, 48]\n",
      "\n",
      "0.23\t[39, 41]\n",
      "\n",
      "0.33\t[39, 48]\n",
      "\n",
      "0.18\t[41, 48]\n",
      "\n",
      "0.14\t[39, 41, 48]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "results = apriori_freq_temsets(large_dataset, 300)\n",
    "retlst, support_data = filter_candidates(results, large_dataset, 300)\n",
    "f = open(\"apriori_itemsets.txt\",\"w+\")\n",
    "print(\"Sup\\tFreq Itemset\\n\")\n",
    "f.write(\"Sup\\tFreq Itemset\\n\")\n",
    "count = 0\n",
    "for itemset, sup in support_data.items():\n",
    "    print(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset)))+'\\n')\n",
    "    if count < len(support_data)-1:\n",
    "        f.write(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset)))+'\\n')\n",
    "    else:\n",
    "        f.write(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset))))\n",
    "    count = count + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Find the closed frequent item sets along with their `support_fraction`. Store the results for the `large_retail.txt` dataset in the file `apriori_closed_itemsets.txt`, in the same format as specified in Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup\tFreq Itemset\n",
      "\n",
      "0.1\t[31]\n",
      "\n",
      "0.22\t[32]\n",
      "\n",
      "0.11\t[36]\n",
      "\n",
      "0.26\t[38]\n",
      "\n",
      "0.6\t[39]\n",
      "\n",
      "0.31\t[41]\n",
      "\n",
      "0.47\t[48]\n",
      "\n",
      "0.11\t[60]\n",
      "\n",
      "0.11\t[65]\n",
      "\n",
      "0.11\t[89]\n",
      "\n",
      "0.14\t[32, 39]\n",
      "\n",
      "0.12\t[32, 48]\n",
      "\n",
      "0.17\t[38, 39]\n",
      "\n",
      "0.11\t[38, 41]\n",
      "\n",
      "0.13\t[38, 48]\n",
      "\n",
      "0.23\t[39, 41]\n",
      "\n",
      "0.33\t[39, 48]\n",
      "\n",
      "0.18\t[41, 48]\n",
      "\n",
      "0.14\t[39, 41, 48]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "large_dataset = load_dataset('large_retail.txt')\n",
    "results = apriori_freq_temsets(large_dataset, 300)\n",
    "retlst, support_data = filter_candidates(results, large_dataset,300)\n",
    "closed_itemsets = {}\n",
    "for itemset, sup in support_data.items():\n",
    "    for item,sup_item in closed_itemsets.copy().items():\n",
    "        if itemset.issuperset(item) and sup>=sup_item:\n",
    "            del closed_itemsets[item]\n",
    "    closed_itemsets[itemset]=sup\n",
    "f = open(\"apriori_closed_itemsets.txt\",\"w+\")\n",
    "print(\"Sup\\tFreq Itemset\\n\")\n",
    "f.write(\"Sup\\tFreq Itemset\\n\")\n",
    "count = 0\n",
    "for itemset, sup in closed_itemsets.items():\n",
    "    print(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset)))+'\\n')\n",
    "    if count < len(closed_itemsets)-1:\n",
    "        f.write(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset)))+'\\n')\n",
    "    else:\n",
    "        f.write(str(round(sup/len(large_dataset),2)) + '\\t' + str(sorted(list(itemset))))\n",
    "    count = count + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Frequent Pattern Tree\n",
    "\n",
    "The FP-Growth Algorithm, proposed by [Han](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf), is an efficient and scalable method for mining the complete set of frequent patterns by pattern fragment growth, using an extended prefix-tree structure for storing compressed and crucial information about frequent patterns named frequent-pattern tree (FP-tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following FPTreeNode class to build your FP Tree. You may add methods to it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables:\n",
    "# self.item_id: item id of the item\n",
    "# self.item_count: item count for this node\n",
    "# self.node_link: next pointer for the linked list that connects\n",
    "#                 nodes of the same item_id (required for the FP-growth algorithm)\n",
    "# self.parent: pointer to the parent node (should be None for the root)\n",
    "# self.children: dictionary for the children (maps from item_id to the FPTreeNode object)\n",
    "# NOTE: root node should have item_id as -1\n",
    "\n",
    "class FPTreeNode:\n",
    "    \n",
    "    def __init__(self, uid, num):\n",
    "        self.item_id = uid\n",
    "        self.item_count = num\n",
    "        self.node_link = None\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "        \n",
    "    def displayTree(self, tab=1):\n",
    "        if self.item_id == -1:\n",
    "            print ('  '*tab, 'root')\n",
    "        else:\n",
    "            print ('  '*tab, 'item_id:', self.item_id, 'item_count:', self.item_count)\n",
    "        for key in sorted(self.children.keys()):\n",
    "            self.children[key].displayTree(tab + 1)\n",
    "            \n",
    "    # helper function for saveToFile\n",
    "    def saveToFile_helper(self, fp, tab=1):\n",
    "        if self.item_id == -1:\n",
    "            print ('  '*tab, 'root', file=fp)\n",
    "        else:\n",
    "            print ('  '*tab, 'item_id:', self.item_id, 'item_count:', self.item_count, file=fp)\n",
    "        for key in sorted(self.children.keys()):\n",
    "            self.children[key].saveToFile_helper(fp, tab + 1)\n",
    "    \n",
    "    # call this to save to file\n",
    "    def saveToFile(self, filename):\n",
    "        with open(filename, 'w') as fp:\n",
    "            self.saveToFile_helper(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Write the function `order_transactions` that takes in the dataset and reorders the items in each transaction based on their frequency (or support counts) and also removes any items that do not meet minsup. This function should return the reordered transactions as well as a dictionary containing the support counts of the frequent 1-itmesets.\n",
    "\n",
    "Note that **item_ids that have equal frequency should be ordered based on the item_id value**.\n",
    "\n",
    "For example,\n",
    "~~~~\n",
    "For the small_retail.txt dataset:\n",
    "{item_id: frequency} -> {1: 6, 2: 7, 3: 6, 4: 2, 5: 2}\n",
    "The transaction [1, 2, 3, 5] should be reordered as [2, 1, 3, 5]\n",
    "Notice the ordering of 1 and 3 (both have 6 occurences in the dataset)\n",
    "\n",
    "If minsup=3, [1, 2, 3, 5] should be reordered as [2, 1, 3] because 5 does not meet minsup.\n",
    "~~~~\n",
    "\n",
    "Note that the **relative order between transactions should not be changed**, they remain in the same order as they appear in the original dataset. For the `small_retail.txt` the transactions would be inserted in the FP Tree in this order:\n",
    "~~~~\n",
    "[2, 1, 5]\n",
    "[2, 4]\n",
    "[2, 3]\n",
    "[2, 1, 4]\n",
    "[1, 3]\n",
    "[2, 3]\n",
    "[1, 3]\n",
    "[2, 1, 3, 5]\n",
    "[2, 1, 3]\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByMinsup(item,freq_cnt):\n",
    "    return freq_cnt[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def order_transactions(dataset, minsup):\n",
    "    freq_cnt = {}\n",
    "    return_freq = {}\n",
    "    new_dataset = []\n",
    "    aux_transaction = []\n",
    "    # your code goes here\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if item not in freq_cnt:\n",
    "                freq_cnt[item] = 1\n",
    "            else:\n",
    "                freq_cnt[item] += 1\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if freq_cnt[item] >= minsup:\n",
    "                aux_transaction.append(item)\n",
    "                aux_transaction.sort(key=lambda x: sortByMinsup(x, freq_cnt),reverse=True)\n",
    "        new_dataset.append(aux_transaction)\n",
    "        aux_transaction = []\n",
    "    #Order freq_cont\n",
    "    for item in sorted(freq_cnt.keys()):\n",
    "        if freq_cnt[item] >= minsup:\n",
    "            return_freq[item] = freq_cnt[item]\n",
    "    return new_dataset, return_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 5], [2, 4], [2, 3], [2, 1, 4], [1, 3], [2, 3], [1, 3], [2, 1, 3, 5], [2, 1, 3]]\n",
      "{1: 6, 2: 7, 3: 6, 4: 2, 5: 2}\n",
      "[4, 5, 1, 3, 2]\n",
      "   root\n",
      "     item_id: 1 item_count: 2\n",
      "       item_id: 3 item_count: 2\n",
      "     item_id: 2 item_count: 7\n",
      "       item_id: 1 item_count: 4\n",
      "         item_id: 3 item_count: 2\n",
      "           item_id: 5 item_count: 1\n",
      "         item_id: 4 item_count: 1\n",
      "         item_id: 5 item_count: 1\n",
      "       item_id: 3 item_count: 2\n",
      "       item_id: 4 item_count: 1\n",
      "False\n",
      "2 1 5 4 3\n",
      "<__main__.FPTreeNode object at 0x11d3b8da0>\n",
      "<__main__.FPTreeNode object at 0x11d3b80b8>\n",
      "<__main__.FPTreeNode object at 0x11d3b8ba8>\n",
      "<__main__.FPTreeNode object at 0x11d3b8da0>\n",
      "<__main__.FPTreeNode object at 0x11d3b80b8>\n",
      "{(2, 1): 1, (2, 1, 3): 1}\n",
      "   root\n",
      "     item_id: 2 item_count: 1\n",
      "       item_id: 1 item_count: 1\n",
      "         item_id: 5 item_count: 1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "new_dataset, freq_cnt = order_transactions(small_dataset, 1)\n",
    "print(new_dataset)\n",
    "print(freq_cnt)\n",
    "\n",
    "print(items_des)\n",
    "root, lst = build_fp_tree(new_dataset)\n",
    "root.displayTree()\n",
    "print(is_single_path(root))\n",
    "print(*lst)\n",
    "x = build_cond_pat_base(lst[5][0])\n",
    "print(x)\n",
    "root, lst = build_fp_tree([[2,1,5]])\n",
    "root.displayTree()\n",
    "print(is_single_path(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Implement the function `build_FP_tree` which takes the ordered itemsets from Q7 and inserts them into the FP Tree. Build the FP Tree for the `large_retail.txt` dataset and write the resulting tree out to `large_fp_tree.txt`.\n",
    "\n",
    "The tree output for `small_retail.txt` dataset is given as follows:\n",
    "~~~~\n",
    "   root\n",
    "     item_id: 1 item_count: 2\n",
    "       item_id: 3 item_count: 2\n",
    "     item_id: 2 item_count: 7\n",
    "       item_id: 1 item_count: 4\n",
    "         item_id: 3 item_count: 2\n",
    "           item_id: 5 item_count: 1\n",
    "         item_id: 4 item_count: 1\n",
    "         item_id: 5 item_count: 1\n",
    "       item_id: 3 item_count: 2\n",
    "       item_id: 4 item_count: 1\n",
    "~~~~\n",
    "This output has been provided to you as `small_fp_tree.txt` for your convenience. You can use the `diff` command in Linux to check your output with the provided output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp_tree (ordered_transactions):\n",
    "    \"\"\"\n",
    "    Builds an FP tree; importantly, links nodes with the same item_id and returns the first \n",
    "    node for each item_id in the first_linked_nodes dictionary\n",
    "    \n",
    "    parameters:\n",
    "        ordered_transactions: the transactions with items sorted in descending rate of occurence\n",
    "\n",
    "    returns:\n",
    "        root (FPTreeNode): the root node of the FP tree\n",
    "        first_linked_nodes (dict of int: FPTreeNode): a map from an item_id to first node of that\n",
    "                                                        item_id; following each node's node_link should \n",
    "                                                        yield every node with that item_id\n",
    "    \"\"\"        \n",
    "    root = FPTreeNode(-1, 0)\n",
    "    first_linked_nodes = {}\n",
    "    count = 0\n",
    "    first_item = None\n",
    "    previous_node = None\n",
    "    \n",
    "    # Your code here\n",
    "    for transaction in ordered_transactions:\n",
    "        for item in transaction:\n",
    "            if first_item==None:\n",
    "                if item not in root.children:\n",
    "                    root.children[item] = FPTreeNode(item, 1)\n",
    "                    root.children[item].parent = -1\n",
    "                    if item not in first_linked_nodes:\n",
    "                        first_linked_nodes[item] = [root.children[item]]\n",
    "                    else:\n",
    "                        first_linked_nodes[item][-1].node_link = root.children[item]\n",
    "                        first_linked_nodes[item].append(root.children[item])\n",
    "                else:\n",
    "                    root.children[item].item_count += 1\n",
    "                first_item = item\n",
    "                previous_node = root.children[first_item]\n",
    "            else:\n",
    "                x = 1\n",
    "                aux_dict = root.children[first_item].children\n",
    "                while x < count:\n",
    "                    aux_dict = aux_dict[transaction[x]].children\n",
    "                    x = x + 1\n",
    "                if item not in aux_dict:\n",
    "                    aux_dict[item] = FPTreeNode(item,1)\n",
    "                    aux_dict[item].node_link = root.children[first_item]\n",
    "                    aux_dict[item].parent = previous_node\n",
    "                    if item not in first_linked_nodes:\n",
    "                        first_linked_nodes[item] = [aux_dict[item]]\n",
    "                    else:\n",
    "                        first_linked_nodes[item][-1].node_link = aux_dict[item]\n",
    "                        first_linked_nodes[item].append(aux_dict[item])\n",
    "                else:\n",
    "                    aux_dict[item].item_count += 1\n",
    "                previous_node = aux_dict[item]\n",
    "            count = count + 1\n",
    "        first_item = None\n",
    "        count = 0\n",
    "    \n",
    "    return root, first_linked_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "new_dataset, freq_cnt = order_transactions(large_dataset, 300)\n",
    "root, lst = build_fp_tree(new_dataset)\n",
    "root.saveToFile(\"large_fp_tree.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Implement `build_cond_pat_base` which take in an FPTreeNode (representing an item) and builds its conditional pattern base. This essentially enumerates all paths in the tree that contain item's item_id.\n",
    "\n",
    "The item (FPTreeNode) that gets passed in to this function will be an FPTreeNode in your header table. This function will first find the path leading to this node. Then it will move to the next FPTreeNode with this same item_id by using `item.node_link` and find the path leading to that node. It will iterate over all items with this item_id by using `item.node_link`.\n",
    "\n",
    "NOTE: The conditional pattern base does not contain item itself. \n",
    "\n",
    "Return the conditional pattern base as a dictionary of tuples to ints representing the path to the item an how many of that item this path gets you to. For example, in the conditional pattern base for item_id 5 in `small_retail.txt` would be: `{(2, 1): 1, (2, 1, 3): 1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cond_pat_base(item):\n",
    "    \"\"\"\n",
    "    Builds a conditional pattern base for an item, starting with the first FPTreeNode of this item \n",
    "    and traversing the node_links to reach all other FPTreeNodes of this item.\n",
    "    \n",
    "    parameters:\n",
    "        item (FPTreeNode): the node we are building a conditional pattern base for\n",
    "    returns:\n",
    "        cond_pat_base (dict of tuple of int: int): conditional pattern base; the patterns with which item's \n",
    "                                                   item_id is found in transactions with and their associated\n",
    "                                                   counts\n",
    "    \"\"\"\n",
    "    cond_pat_base = {}\n",
    "    # your code goes here\n",
    "    list_parent = []\n",
    "    aux_item = item\n",
    "    while aux_item != None:\n",
    "        aux_parent = aux_item.parent\n",
    "        while aux_parent != -1:\n",
    "            list_parent.append(aux_parent.item_id)\n",
    "            aux_parent = aux_parent.parent\n",
    "        if list_parent != []:\n",
    "            cond_pat_base[tuple(list_parent[::-1])] = aux_item.item_count\n",
    "        list_parent = []\n",
    "        aux_item = aux_item.node_link\n",
    "    \n",
    "    return cond_pat_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** Implement the function `is_single_path` that takes in an FP-Tree root node and determines whether or not it is a single-path tree (no branches). Return True for a single-path tree, and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_path(fp_tree):  \n",
    "    # your code goes here\n",
    "    aux_root = fp_tree\n",
    "    while len(aux_root.children) != 0:\n",
    "        if len(aux_root.children) > 1:\n",
    "            return False\n",
    "        only_children = list(aux_root.children.keys())[0]  \n",
    "        aux_root = aux_root.children[only_children]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Now, implement the FP Growth algorithm to mine the FP-Tree for frequent patterns. \n",
    "\n",
    "Starting from the least frequent 1-itemsets:\n",
    "    1. build the conditional pattern base\n",
    "    2. build a new FP tree from the conditional pattern base\n",
    "You then need to handle the following cases:\n",
    "    1. the newly constructed FP tree has no children\n",
    "    2. the newly constructed FP tree is a single path tree\n",
    "    3. the newly constructed FP tree is not a single path tree\n",
    "    \n",
    "    If the new FP tree is not a single path tree, you will need to recurse.\n",
    "    If the new FP tree is a single path tree, you will need to enumerate the subsets of the path.\n",
    "\n",
    "You may add addition helper functions as you see fit. But remember you are only allowed to mine the FP tree that you generate in Q8. You *cannot use the dataset* other than in the first line of code provided here.\n",
    "\n",
    "**Store** the output of mining the `large_retail.txt` dataset with a minsup=300 in the file `fp_growth_itemsets.txt`.\n",
    "\n",
    "Hint: You can check your answer by comparing it to the output of your Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(cond_path):\n",
    "    dataset = []\n",
    "    transaction = []\n",
    "    for path, count in cond_path.items():\n",
    "        for x in range(0,count):\n",
    "            for element in path:\n",
    "                transaction.append(element)\n",
    "            dataset.append(transaction)\n",
    "            transaction = []\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursion(dataset,minsup,final_freq_itemsets,y):\n",
    "    ordered_transactions, freq_items = order_transactions(dataset, minsup)\n",
    "    fp_tree, link_table = build_fp_tree(ordered_transactions)\n",
    " \n",
    "    if len(fp_tree.children) == 0:\n",
    "        if fp_tree.item_id != -1:\n",
    "            if y not in final_freq_itemsets:\n",
    "                final_freq_itemsets[y] = [([fp_tree.item_id], fp_tree.item_count)]\n",
    "            else:\n",
    "                final_freq_itemsets[y] += [(fp_tree.item_id, fp_tree.item_count)]\n",
    "        return\n",
    "    if is_single_path(fp_tree) == True:\n",
    "        aux_root = fp_tree\n",
    "        lst_path = []\n",
    "        while len(aux_root.children) != 0:\n",
    "            only_children = list(aux_root.children.keys())[0]\n",
    "            lst_path.append(only_children)\n",
    "            aux_root = aux_root.children[only_children]\n",
    "        if y not in final_freq_itemsets:\n",
    "            final_freq_itemsets[y] = [(lst_path, aux_root.item_count)]\n",
    "        else:\n",
    "            final_freq_itemsets[y] += [(lst_path, aux_root.item_count)]\n",
    "    else:\n",
    "        items_des = []\n",
    "        for val in sorted(freq_items,key=freq_items.get):\n",
    "            items_des.append(val)\n",
    "        for x in items_des:\n",
    "            first_node = link_table[x][0]\n",
    "            cond_pat = build_cond_pat_base(first_node.node_link)\n",
    "            datas = make_dataset(cond_pat)\n",
    "            recursion(datas,minsup,final_freq_itemsets,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_growth(dataset, minsup):\n",
    "    ordered_transactions, freq_items = order_transactions(dataset, minsup)\n",
    "    fp_tree, link_table = build_fp_tree(ordered_transactions)\n",
    "    final_freq_itemsets = {}\n",
    "    \n",
    "    if len(fp_tree.children) != 0:\n",
    "        items_des = []\n",
    "        for val in sorted(freq_items,key=freq_items.get):\n",
    "            items_des.append(val)\n",
    "        for x in items_des:\n",
    "            first_node = link_table[x][0]\n",
    "            cond_pat = build_cond_pat_base(first_node)\n",
    "            datas = make_dataset(cond_pat)\n",
    "            recursion(datas,minsup,final_freq_itemsets,x)\n",
    "    return final_freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = load_dataset('large_retail.txt')\n",
    "final_freq_itemsets = fp_growth(large_dataset, 300)\n",
    "f = open(\"fp_growth_itemsets.txt\",\"w+\")\n",
    "f.write(\"Sup\\tFreq Itemset\\n\")\n",
    "for val in final_freq_itemsets.keys():\n",
    "    tup = final_freq_itemsets[val]\n",
    "    lst = tup[0][0]\n",
    "    count = tup[0][1]\n",
    "    #print(str(round(count/len(small_dataset),2)) + '\\t' + str(sorted(list(lst)+[val]))+'\\n')\n",
    "    f.write(str(round(count/len(large_dataset),2)) + '\\t' + str(sorted(list(lst)+[val]))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Extra Credit (+5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Generate the rules having confidence above `min_conf = 0.5` using the frequent itemsets generated in Q5. Display the rules in the form of a table.\n",
    "\n",
    "Sample table format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Conf    Rule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "Note that rule confidence should be rounded to the nearest 2 decimal places (use `round(conf, 2)`). This table should also be tab (`'\\t'`) separated. The rules should be displayed in the sorted order. If a rule is given as `LHS -> RHS` then the rules for which `len(LHS)` is lesser should appear first. If the `len(LHS)` is equal for two rules then rules for which `len(RHS)` is lesser should appear first. If both `len(LHS)` and `len(RHS)` is equal then the rules should be sorted based on LHS first and then based on RHS.\n",
    "\n",
    "~~~~\n",
    "Note:\n",
    "LHS (Left Hand Side)\n",
    "RHS (Right Hand Side)\n",
    "~~~~\n",
    "\n",
    "For eg.\n",
    "~~~~\n",
    "[3] -> [2] should come before [1, 3] -> [4]\n",
    "[4] -> [2] should come before [2] -> [3, 4]\n",
    "[1, 3] -> [2] should come before [1, 5] -> [2]\n",
    "[1, 2] -> [3] should come before [1, 2] -> [5]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The sample output for the `small_retail.txt` dataset with `min_conf = 0.5` is:\n",
    "\n",
    "~~~~\n",
    "Sup\t Conf\tRule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.44\t0.67\t[1] -> [3]\n",
    "0.44\t0.57\t[2] -> [1]\n",
    "0.44\t0.57\t[2] -> [3]\n",
    "0.44\t0.67\t[3] -> [1]\n",
    "0.44\t0.67\t[3] -> [2]\n",
    "0.22\t1.0\t [4] -> [2]\n",
    "0.22\t1.0\t [5] -> [1]\n",
    "0.22\t1.0\t [5] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t0.5\t [1, 2] -> [3]\n",
    "0.22\t0.5\t [1, 2] -> [5]\n",
    "0.22\t0.5\t [1, 3] -> [2]\n",
    "0.22\t1.0\t [1, 5] -> [2]\n",
    "0.22\t0.5\t [2, 3] -> [1]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `apriori_rules.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_rules.txt` for your convenience.\n",
    "\n",
    "**Hint:** You shouldn't traverse the entire dataset to compute the confidence for a rule since you have already computed the `support_data` for all the frequent itemsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    \n",
    "    def __init__(self, sup, conf, fst, snd):\n",
    "        self.support = sup\n",
    "        self.confidence = conf\n",
    "        self.first = fst\n",
    "        self.second = snd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup\tFreq Itemset\n",
      "\n",
      "0.14\t0.65\t[32] -> [39]\n",
      "0.12\t0.55\t[32] -> [48]\n",
      "0.17\t0.67\t[38] -> [39]\n",
      "0.33\t0.55\t[39] -> [48]\n",
      "0.23\t0.74\t[41] -> [39]\n",
      "0.18\t0.57\t[41] -> [48]\n",
      "0.33\t0.7\t[48] -> [39]\n",
      "0.14\t0.61\t[41, 39] -> [48]\n",
      "0.14\t0.8\t[48, 41] -> [39]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "large_dataset = load_dataset('large_retail.txt')\n",
    "results = apriori_freq_temsets(large_dataset, 300)\n",
    "retlst, support_data = filter_candidates(results, large_dataset, 300)\n",
    "min_conf = 0.5\n",
    "rules = []\n",
    "\n",
    "f = open(\"apriori_rules.txt\",\"w+\")\n",
    "\n",
    "for itemset, sup in support_data.items():\n",
    "    if len(list(itemset)) > 1:\n",
    "        for x in itemset:\n",
    "            conf = support_data[frozenset(itemset)]/support_data[frozenset(itemset).difference(frozenset([x]))]\n",
    "            if conf >= min_conf:\n",
    "                support = sup/len(large_dataset)\n",
    "                rules.append(Rule(support,conf,frozenset(itemset).difference(frozenset([x])),[x]))\n",
    "\n",
    "rules.sort(key=lambda t: (len(t.first), list(t.first), list(t.second)))\n",
    "print(\"Sup\\tFreq Itemset\\n\")\n",
    "f.write(\"Sup\\tFreq Itemset\\n\")\n",
    "count = 0\n",
    "for rule in rules:\n",
    "    print(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)))\n",
    "    if count < len(rules)-1:\n",
    "        f.write(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)) + '\\n')\n",
    "    else:\n",
    "        f.write(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)))\n",
    "    count = count +1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup\tFreq Itemset\n",
      "\n",
      "0.44\t0.67\t[1] -> [2]\n",
      "0.44\t0.67\t[1] -> [3]\n",
      "0.44\t0.57\t[2] -> [1]\n",
      "0.44\t0.57\t[2] -> [3]\n",
      "0.44\t0.67\t[3] -> [1]\n",
      "0.44\t0.67\t[3] -> [2]\n",
      "0.22\t1.0\t[4] -> [2]\n",
      "0.22\t1.0\t[5] -> [1]\n",
      "0.22\t1.0\t[5] -> [2]\n",
      "0.22\t0.5\t[1, 2] -> [3]\n",
      "0.22\t0.5\t[1, 2] -> [5]\n",
      "0.22\t0.5\t[1, 3] -> [2]\n",
      "0.22\t1.0\t[1, 5] -> [2]\n",
      "0.22\t0.5\t[2, 3] -> [1]\n",
      "0.22\t1.0\t[2, 5] -> [1]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "large_dataset = load_dataset('small_retail.txt')\n",
    "results = apriori_freq_temsets(large_dataset, 2)\n",
    "retlst, support_data = filter_candidates(results, large_dataset, 2)\n",
    "min_conf = 0.5\n",
    "rules = []\n",
    "\n",
    "f = open(\"s_apriori_rules.txt\",\"w+\")\n",
    "\n",
    "for itemset, sup in support_data.items():\n",
    "    if len(list(itemset)) > 1:\n",
    "        for x in itemset:\n",
    "            conf = support_data[frozenset(itemset)]/support_data[frozenset(itemset).difference(frozenset([x]))]\n",
    "            if conf >= min_conf:\n",
    "                support = sup/len(large_dataset)\n",
    "                rules.append(Rule(support,conf,frozenset(itemset).difference(frozenset([x])),[x]))\n",
    "\n",
    "rules.sort(key=lambda t: (len(t.first), list(t.first), list(t.second)))\n",
    "print(\"Sup\\tFreq Itemset\\n\")\n",
    "f.write(\"Sup\\Conf\\tRule\\n\")\n",
    "count = 0\n",
    "for rule in rules:\n",
    "    print(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)))\n",
    "    if count < len(rules)-1:\n",
    "        f.write(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)) + '\\n')\n",
    "    else:\n",
    "        f.write(str(round(rule.support,2)) + '\\t' + str(round(rule.confidence,2)) + '\\t' + str(list(rule.first)) + ' -> '+ str(list(rule.second)))\n",
    "    count = count +1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
